# -*- coding: utf-8 -*-
"""trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1USSv5v4nNPev2I6cLZnxus07tqwSmDO2
"""

!pip uninstall tensorflow
!pip install tensorflow==2.0.0

import json
import os
import sys
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow import keras
from google.colab import drive

print(tf.__version__)
print(sys.version)

drive.mount('/content/drive')

imdb_dataset, imdb_info = tfds.load('imdb_reviews/subwords8k', with_info=True,
                          as_supervised=True)

directory = '/content/drive/My Drive/Bay Area Hacks/articles_jsons'

checkpoint_path = '/content/drive/My Drive/Bay Area Hacks/checkpoint.ckpt'
checkpoint_dir = os.path.dirname(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

save_path = '/content/drive/My Drive/Bay Area Hacks/saved_binary_model/my_model'

null = None
 
eval_arr = []
 
print("starting")
 
print("importing all json data into numpy array")
 
i = 0
 
for subdir, dirs, files in os.walk(directory):
  for file in files:
    # if subdir == '/content/drive/My Drive/Bay Area Hacks/articles_jsons/washingtonpost_articles_jsons':
    #   break

    print(i)

    if i  > 1499:
      break
    if file.endswith(".json"):
 
      json_data = json.load(open(subdir + '/' + file))
 
      eval_arr.append(eval(json_data))
 
      i+=1
 
np_arr = np.asarray(eval_arr)
 
print('imported all json data into numpy array')
 
print("developing corpus_generator encoder")

encoder = imdb_info.features['text'].encoder
 
print("developed corpus_generator encoder")

print("encoding numpy data")
 
three_quarters = int(len(np_arr) * (3 / 4))
 
np_features_train = np.zeros((three_quarters + 1, 2**13), dtype=np.float32)
np_labels_train = np.zeros((three_quarters + 1, 1), dtype=np.float32)
 
np_features_test = np.zeros((len(np_arr) - three_quarters, 2**13), dtype=np.float32)
np_labels_test = np.zeros((len(np_arr) - three_quarters, 1), dtype=np.float32)
 
i, index = 0, 0
 
for article in np_arr:
  encoded_title = ''
  encoded_text = ''
  if ((i % 4) != 0):

    index = int(i - int(i / 4))

    if (article['title'] != None):
      encoded_title = (encoder.encode(article['title']))
    else:
      encoded_title = (encoder.encode(' '))
 
    if (article['text'] != None):
      encoded_text = (encoder.encode(article['text']))
    else:
      encoded_text = (encoder.encode(' '))
 
    j = 0
 
    encoded_title.extend(encoded_text)
 
    for title in encoded_title:
      if (j < 2**13):
 
        np_features_train[index][j] = title
      j+=1
 
    np_labels_train[index][0] = article['positive']
    # np_labels_train[index][0] = article['negative']
    # np_labels_train[index][0] = article['political']
    # np_labels_train[index][0] = article['neutral']
  else:

    index = int(i * (1/4))

    if (article['title'] != None):
      encoded_title = (encoder.encode(article['title']))
    else:
      encoded_title = (encoder.encode(' '))
 
    if (article['text'] != None):
      encoded_text = (encoder.encode(article['text']))
    else:
      encoded_text = (encoder.encode(' '))
 
    j = 0
 
    encoded_title.extend(encoded_text)
 
    for title in encoded_title:
      if (j < 2**13):
 
        np_features_test[index][j] = title
      j+=1
 
    np_labels_test[index][0] = article['positive']
    # np_labels_test[index][0] = article['negative']
    # np_labels_test[index][0] = article['political']
    # np_labels_test[index][0] = article['neutral']
 
  i+=1
 
print("encoded numpy data")

print(np_features_train.shape)
print(np_labels_train.shape)
 
print("preparing for ML...")

train_dataset = tf.data.Dataset.from_tensor_slices((np_features_train, np_labels_train))
test_dataset = tf.data.Dataset.from_tensor_slices((np_features_test, np_labels_test))
 
train_dataset = train_dataset.shuffle(len(np_features_train))
train_dataset = train_dataset.batch(4)
test_dataset = test_dataset.batch(4)
 
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(encoder.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
  ])
 
# model.compile(optimizer='adam',
#     loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
#     metrics=['accuracy'])

model.compile(optimizer='adam',
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=['accuracy'])
 
model.fit(train_dataset, epochs=12, steps_per_epoch=8,
          validation_data=test_dataset,
          validation_steps=4,
          callbacks=[cp_callback])

test_loss, test_acc = model.evaluate(test_dataset)

print('Test Loss: {}'.format(test_loss))
print('Test Accuracy: {}'.format(test_acc))

model.save(save_path)

('saved model')

('reloading and testing model')

reloaded_model = tf.keras.models.load_model(save_path)

reloaded_test_loss, reloaded_test_acc = reloaded_model.evaluate(test_dataset)

print('Test Loss: {}'.format(reloaded_test_loss))
print('Test Accuracy: {}'.format(reloaded_test_acc))

('reloaded and tested model')

